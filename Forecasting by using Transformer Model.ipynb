{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6b5d9c6-c194-49bb-b39b-8f25aedd24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from scipy.stats import randint\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import pickle\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae05284e-dcdb-4cc0-b5e7-5cad72b31166",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = pd.read_excel('GH_UK_2011_to_2013.xlsx', sheet_name='sales_UK_2011_to_2013') \n",
    "other_features = pd.read_excel('GH_UK_2011_to_2013.xlsx', sheet_name='Other_Features')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8210444c-10e4-49b4-9b43-02f5c981f51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 440256 entries, 0 to 440255\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   invoice_no    440256 non-null  int64         \n",
      " 1   stock_code    440256 non-null  object        \n",
      " 2   description   440256 non-null  object        \n",
      " 3   quantity      440256 non-null  int64         \n",
      " 4   invoice_date  440256 non-null  datetime64[ns]\n",
      " 5   unit_price    440256 non-null  float64       \n",
      " 6   customer_id   440256 non-null  int64         \n",
      " 7   country       440256 non-null  object        \n",
      " 8   total_sales   440256 non-null  float64       \n",
      " 9   year          440256 non-null  int64         \n",
      " 10  month         440256 non-null  int64         \n",
      " 11  season        440256 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(5), object(4)\n",
      "memory usage: 40.3+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36 entries, 0 to 35\n",
      "Data columns (total 7 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   year                         36 non-null     int64  \n",
      " 1   month                        36 non-null     int64  \n",
      " 2   CPI                          36 non-null     float64\n",
      " 3   GDP                          36 non-null     float64\n",
      " 4   VALUE OF RETAIL SALES        36 non-null     float64\n",
      " 5   VALUE OF Internet Sales      36 non-null     float64\n",
      " 6   Holiday and Seasonal Trends  22 non-null     object \n",
      "dtypes: float64(4), int64(2), object(1)\n",
      "memory usage: 2.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sales_data.info())\n",
    "print(other_features.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f00a9ca-661b-48e9-926f-c9b4abea2379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   invoice_no stock_code                     description  quantity  \\\n",
      "0      558371      20723        strawberry charlotte bag        37   \n",
      "1      580610      20726              lunch bag woodland         4   \n",
      "2      547772      22293  hanging chick green decoration         2   \n",
      "\n",
      "  invoice_date  unit_price  customer_id         country  total_sales  year  \\\n",
      "0   2011-06-28        2.46            0  United Kingdom        91.02  2011   \n",
      "1   2011-12-05        4.13            0  United Kingdom        16.52  2011   \n",
      "2   2011-03-25        1.45        17979  United Kingdom         2.90  2011   \n",
      "\n",
      "   month  season  \n",
      "0      6  Summer  \n",
      "1     12  Winter  \n",
      "2      3  Spring  \n",
      "Distinct stock_code count: 3765\n",
      "Distinct description count: 3852\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 379 entries, 7 to 437548\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   stock_code   379 non-null    object\n",
      " 1   description  379 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.9+ KB\n",
      "None\n",
      "48     [beaded chandelier t-light holder, gemstone chandelier t-light holder]\n",
      "111             [pack of 12 vintage doily tissues, pack of 12 doiley tissues]\n",
      "23            [musical zinc heart decoration , christmas musical zinc heart ]\n",
      "56                    [zinc  star t-light holder , zinc star t-light holder ]\n",
      "7                        [glitter heart decoration, glitter christmas heart ]\n",
      "Name: description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows of the DataFrame\n",
    "print(sales_data.head(3))\n",
    "\n",
    "# Count distinct values\n",
    "distinct_count = sales_data['stock_code'].nunique()\n",
    "print(\"Distinct stock_code count:\", distinct_count)\n",
    "\n",
    "distinct_count2 = sales_data['description'].nunique()\n",
    "print(\"Distinct description count:\", distinct_count2)\n",
    "\n",
    "# Investigate the mismatch\n",
    "# Create a DataFrame with unique combinations of stock_code and description\n",
    "unique_combinations = sales_data[['stock_code', 'description']].drop_duplicates()\n",
    "\n",
    "# check for any stock_code that has multiple descriptions\n",
    "mismatched_stock_codes = unique_combinations.groupby('stock_code').filter(lambda x: len(x) > 1)\n",
    "print(mismatched_stock_codes.info())\n",
    "\n",
    "# further investigation\n",
    "mismatched_codes_list = mismatched_stock_codes.iloc[:, 0].tolist() \n",
    "\n",
    "# Filter the unique_combinations DataFrame for stock codes in mismatched_codes_list\n",
    "filtered_data = unique_combinations[unique_combinations['stock_code'].isin(mismatched_codes_list)]\n",
    "\n",
    "# Group by stock_code and aggregate descriptions\n",
    "grouped_data = filtered_data.groupby('stock_code')['description'].apply(list).reset_index()\n",
    "\n",
    "# Set display options to show the full text of the DataFrame\n",
    "pd.set_option('display.max_colwidth', None)  # None means no limit on column width\n",
    "\n",
    "# Print the grouped data\n",
    "print(grouped_data.sample(5)['description'])\n",
    "\n",
    "# Reset the display options to default after printing\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9183fa8b-b6f9-495f-83e2-6ba8d3fbf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the sales data by `stock_code` and `invoice_date` to get daily sales\n",
    "daily_sales = sales_data.groupby(['invoice_date', 'stock_code']).agg({'total_sales': 'sum'}).reset_index()\n",
    "\n",
    "# Extract year, month, and day from `invoice_date`\n",
    "daily_sales['year'] = daily_sales['invoice_date'].dt.year\n",
    "daily_sales['month'] = daily_sales['invoice_date'].dt.month\n",
    "daily_sales['day'] = daily_sales['invoice_date'].dt.day  # Extract day\n",
    "\n",
    "# Ensure 'year' and 'month' in other_features are integers\n",
    "other_features['year'] = other_features['year'].astype(int)\n",
    "other_features['month'] = other_features['month'].astype(int)\n",
    "\n",
    "# Expand monthly data into daily data dynamically\n",
    "expanded_rows = []\n",
    "\n",
    "for _, row in other_features.iterrows():\n",
    "    year, month = row['year'], row['month']\n",
    "    days_in_month = calendar.monthrange(year, month)[1]  # Get actual number of days in the month\n",
    "    \n",
    "    # Create daily entries for each day in the month\n",
    "    for day in range(1, days_in_month + 1):\n",
    "        new_row = row.copy()\n",
    "        new_row['day'] = day  # Assign day\n",
    "        \n",
    "        # Divide numeric values by the number of days to distribute them evenly\n",
    "        for col in ['CPI', 'GDP', 'VALUE OF RETAIL SALES', 'VALUE OF Internet Sales ']:\n",
    "            new_row[col] /= days_in_month  # Normalize monthly values\n",
    "        \n",
    "        # Assign Holiday & Seasonal Trends **only on the correct date**\n",
    "        if day == 1:  \n",
    "            new_row['Holiday and Seasonal Trends'] = row['Holiday and Seasonal Trends']  # Assign only to 1st\n",
    "        else:\n",
    "            new_row['Holiday and Seasonal Trends'] = np.nan  # Keep other days empty\n",
    "        \n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Create a new DataFrame with daily features\n",
    "daily_features = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Merge daily features with daily sales using year, month, and day\n",
    "daily_sales = daily_sales.merge(daily_features, on=['year', 'month', 'day'], how='left')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "daily_sales.drop(columns=['year', 'month', 'day'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e662b0b8-9638-4b0c-8733-618fe3e572aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 338669 entries, 0 to 338668\n",
      "Data columns (total 8 columns):\n",
      " #   Column                       Non-Null Count   Dtype         \n",
      "---  ------                       --------------   -----         \n",
      " 0   invoice_date                 338669 non-null  datetime64[ns]\n",
      " 1   stock_code                   338669 non-null  object        \n",
      " 2   total_sales                  338669 non-null  float64       \n",
      " 3   CPI                          338669 non-null  float64       \n",
      " 4   GDP                          338669 non-null  float64       \n",
      " 5   VALUE OF RETAIL SALES        338669 non-null  float64       \n",
      " 6   VALUE OF Internet Sales      338669 non-null  float64       \n",
      " 7   Holiday and Seasonal Trends  6766 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(5), object(2)\n",
      "memory usage: 20.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(daily_sales.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cb65328-0efe-420d-bc27-dd0e768c02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'stock_code' and sum 'total_sales'\n",
    "grouped_sales = daily_sales.groupby('stock_code')['total_sales'].sum().reset_index()\n",
    "\n",
    "# Step 2: Calculate the threshold for the top 70%\n",
    "threshold = grouped_sales['total_sales'].quantile(0.7)\n",
    "\n",
    "# Step 3: Filter the DataFrame for the top items\n",
    "top_items = grouped_sales[grouped_sales['total_sales'] >= threshold]\n",
    "\n",
    "\n",
    "top_items = daily_sales[daily_sales['stock_code'].isin(top_items['stock_code'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da0dd0e-5928-418f-9fce-7ea0574ac11e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 227962 entries, 1 to 338646\n",
      "Data columns (total 8 columns):\n",
      " #   Column                       Non-Null Count   Dtype         \n",
      "---  ------                       --------------   -----         \n",
      " 0   invoice_date                 227962 non-null  datetime64[ns]\n",
      " 1   stock_code                   227962 non-null  object        \n",
      " 2   total_sales                  227962 non-null  float64       \n",
      " 3   CPI                          227962 non-null  float64       \n",
      " 4   GDP                          227962 non-null  float64       \n",
      " 5   VALUE OF RETAIL SALES        227962 non-null  float64       \n",
      " 6   VALUE OF Internet Sales      227962 non-null  float64       \n",
      " 7   Holiday and Seasonal Trends  4721 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(5), object(2)\n",
      "memory usage: 15.7+ MB\n",
      "None\n",
      "Index(['invoice_date', 'stock_code', 'total_sales', 'CPI', 'GDP',\n",
      "       'VALUE OF RETAIL SALES', 'VALUE OF Internet Sales ',\n",
      "       'Holiday and Seasonal Trends'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(top_items.info())\n",
    "print(top_items.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b3898f7-be21-44d0-805f-2e74a6cc42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/r79cd99d3673p095x2_48c_00000gn/T/ipykernel_84775/3046846140.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_items['invoice_date'] = pd.to_datetime(top_items['invoice_date'])\n",
      "/Users/doradeng/tensorflow-test/env/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "# Convert 'invoice_date' to datetime\n",
    "top_items['invoice_date'] = pd.to_datetime(top_items['invoice_date'])\n",
    "\n",
    "# Step 2: Aggregate monthly total_sales for each stock_code while keeping all columns\n",
    "monthly_sales = top_items.groupby(['stock_code', top_items['invoice_date'].dt.to_period('M')]).agg(\n",
    "    total_sales=('total_sales', 'sum'),\n",
    "    CPI=('CPI', 'first'),\n",
    "    GDP=('GDP', 'first'),\n",
    "    VALUE_OF_RETAIL_SALES=('VALUE OF RETAIL SALES', 'first'),\n",
    "    VALUE_OF_Internet_Sales=('VALUE OF Internet Sales ', 'first'),\n",
    "    Holiday_and_Seasonal_Trends=('Holiday and Seasonal Trends', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Convert the period back to a timestamp for the index\n",
    "monthly_sales['invoice_date'] = monthly_sales['invoice_date'].dt.to_timestamp()\n",
    "\n",
    "# Set the 'invoice_date' as the index\n",
    "monthly_sales.set_index('invoice_date', inplace=True)\n",
    "\n",
    "# Step 3: Prepare features and labels\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "monthly_sales['stock_code'] = monthly_sales['stock_code'].astype(str)\n",
    "monthly_sales['stock_code'] = label_encoder.fit_transform(monthly_sales['stock_code'])\n",
    "\n",
    "# One-hot encoding for 'Holiday and Seasonal Trends'\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "holiday_encoded = encoder.fit_transform(monthly_sales[['Holiday_and_Seasonal_Trends']])\n",
    "holiday_df = pd.DataFrame(holiday_encoded, columns=encoder.get_feature_names_out(['Holiday_and_Seasonal_Trends']))\n",
    "\n",
    "# Reset index to avoid InvalidIndexError\n",
    "monthly_sales.reset_index(inplace=True)\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "monthly_sales = pd.concat([monthly_sales, holiday_df], axis=1)\n",
    "\n",
    "# Drop the original 'Holiday and Seasonal Trends' column\n",
    "monthly_sales.drop(columns=['Holiday_and_Seasonal_Trends'], inplace=True)\n",
    "\n",
    "# Set the 'invoice_date' back as the index if needed\n",
    "monthly_sales.set_index('invoice_date', inplace=True)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = monthly_sales[['stock_code', 'CPI', 'GDP', 'VALUE_OF_RETAIL_SALES', 'VALUE_OF_Internet_Sales'] + list(holiday_df.columns)].values\n",
    "y = monthly_sales['total_sales'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afae029d-68d3-4797-ab4d-15a8a7e14e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 33570 entries, 2011-01-01 to 2013-03-01\n",
      "Data columns (total 13 columns):\n",
      " #   Column                                                       Non-Null Count  Dtype  \n",
      "---  ------                                                       --------------  -----  \n",
      " 0   stock_code                                                   33570 non-null  int64  \n",
      " 1   total_sales                                                  33570 non-null  float64\n",
      " 2   CPI                                                          33570 non-null  float64\n",
      " 3   GDP                                                          33570 non-null  float64\n",
      " 4   VALUE_OF_RETAIL_SALES                                        33570 non-null  float64\n",
      " 5   VALUE_OF_Internet_Sales                                      33570 non-null  float64\n",
      " 6   Holiday_and_Seasonal_Trends_Back to School                   33570 non-null  float64\n",
      " 7   Holiday_and_Seasonal_Trends_Bank Holiday                     33570 non-null  float64\n",
      " 8   Holiday_and_Seasonal_Trends_Black Friday                     33570 non-null  float64\n",
      " 9   Holiday_and_Seasonal_Trends_Christmas                        33570 non-null  float64\n",
      " 10  Holiday_and_Seasonal_Trends_Easter                           33570 non-null  float64\n",
      " 11  Holiday_and_Seasonal_Trends_Queen’s Diamond Jubilee Holiday  33570 non-null  float64\n",
      " 12  Holiday_and_Seasonal_Trends_None                             33570 non-null  float64\n",
      "dtypes: float64(12), int64(1)\n",
      "memory usage: 3.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(monthly_sales.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceb19a86-374c-41da-a1a9-9ebc7d029b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocess for transformer model \n",
    "\n",
    "#**Create Sequences**: Define a function to create sequences for time series forecasting.\n",
    "\n",
    "def create_sequences(X, y, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        sequences.append(X[i:i + seq_length])\n",
    "        labels.append(y[i + seq_length])  # Predicting the next time step\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "seq_length = 10 \n",
    "X_, y_ = create_sequences(X, y, seq_length)\n",
    "   \n",
    "  \n",
    "# Split the data into training, validation, and testing sets based on time\n",
    "train_size = int(len(X_) * 0.6)  # 60% for training\n",
    "val_size = int(len(X_) * 0.2)     # 20% for validation\n",
    "test_size = len(X_) - train_size - val_size  # Remaining 20% for testing\n",
    "\n",
    "TR_X_train, TR_y_train = X_[:train_size], y_[:train_size]\n",
    "TR_X_val, TR_y_val = X_[train_size:train_size + val_size], y_[train_size:train_size + val_size]\n",
    "TR_X_test, TR_y_test = X_[train_size + val_size:], y_[train_size + val_size:]\n",
    "\n",
    "# Note: No random_state is used here to maintain the temporal order\n",
    "\n",
    "scaler_y_ = MinMaxScaler()\n",
    "TR_y_train = scaler_y_.fit_transform(TR_y_train.reshape(-1, 1)).flatten()\n",
    "TR_y_val = scaler_y_.transform(TR_y_val.reshape(-1, 1)).flatten()\n",
    "TR_y_test = scaler_y_.transform(TR_y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler_X_ = MinMaxScaler()\n",
    "\n",
    "\n",
    "scaler_X_.fit(TR_X_train.reshape(-1, TR_X_train.shape[-1]))  # Fit on training set only\n",
    "\n",
    "TR_X_train_scaled = scaler_X_.transform(TR_X_train.reshape(-1, TR_X_train.shape[-1])).reshape(TR_X_train.shape)\n",
    "TR_X_val_scaled = scaler_X_.transform(TR_X_val.reshape(-1, TR_X_val.shape[-1])).reshape(TR_X_val.shape)\n",
    "TR_X_test_scaled = scaler_X_.transform(TR_X_test.reshape(-1, TR_X_test.shape[-1])).reshape(TR_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de3691f3-9a79-4768-bb10-004c406066c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:17:57.567654: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-14 04:17:57.567711: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:17:59.054666: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-14 04:17:59.153764: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - ETA: 0s - loss: 1.7225 - mae: 0.5857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:18:12.587557: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 16s 22ms/step - loss: 1.7225 - mae: 0.5857 - val_loss: 1.0662 - val_mae: 0.2779 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 1.2233 - mae: 0.4747 - val_loss: 0.8564 - val_mae: 0.3476 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.8633 - mae: 0.3796 - val_loss: 0.5748 - val_mae: 0.2003 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.6294 - mae: 0.3306 - val_loss: 0.4116 - val_mae: 0.1735 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.4527 - mae: 0.2896 - val_loss: 0.2928 - val_mae: 0.1667 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.3213 - mae: 0.2540 - val_loss: 0.2018 - val_mae: 0.1509 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.2252 - mae: 0.2251 - val_loss: 0.1372 - val_mae: 0.1437 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.1545 - mae: 0.1991 - val_loss: 0.0905 - val_mae: 0.1325 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.1045 - mae: 0.1764 - val_loss: 0.0611 - val_mae: 0.1324 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0706 - mae: 0.1576 - val_loss: 0.0384 - val_mae: 0.1152 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0463 - mae: 0.1372 - val_loss: 0.0191 - val_mae: 0.0701 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0279 - mae: 0.1041 - val_loss: 0.0165 - val_mae: 0.0889 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0155 - mae: 0.0617 - val_loss: 0.0064 - val_mae: 0.0299 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0102 - mae: 0.0526 - val_loss: 0.0046 - val_mae: 0.0227 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0070 - mae: 0.0470 - val_loss: 0.0037 - val_mae: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0050 - mae: 0.0405 - val_loss: 0.0035 - val_mae: 0.0210 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0044 - mae: 0.0404 - val_loss: 0.0036 - val_mae: 0.0302 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0043 - mae: 0.0428 - val_loss: 0.0027 - val_mae: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0033 - mae: 0.0373 - val_loss: 0.0024 - val_mae: 0.0178 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0024 - mae: 0.0298 - val_loss: 0.0022 - val_mae: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0018 - mae: 0.0247 - val_loss: 0.0020 - val_mae: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0016 - mae: 0.0228 - val_loss: 0.0020 - val_mae: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0016 - mae: 0.0224 - val_loss: 0.0019 - val_mae: 0.0174 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0026 - mae: 0.0325 - val_loss: 0.0019 - val_mae: 0.0185 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0018 - mae: 0.0261 - val_loss: 0.0019 - val_mae: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0017 - mae: 0.0241 - val_loss: 0.0019 - val_mae: 0.0172 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0016 - mae: 0.0221 - val_loss: 0.0019 - val_mae: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0015 - mae: 0.0208 - val_loss: 0.0019 - val_mae: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "502/504 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0200\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0014 - mae: 0.0200 - val_loss: 0.0019 - val_mae: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0014 - mae: 0.0193 - val_loss: 0.0019 - val_mae: 0.0180 - lr: 5.0000e-05\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:23:07.433541: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-14 04:23:07.525609: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - ETA: 0s - loss: 2.6684 - mae: 0.9533"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:23:17.625360: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 12s 22ms/step - loss: 2.6684 - mae: 0.9533 - val_loss: 1.0271 - val_mae: 0.1353 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 1.9086 - mae: 0.7738 - val_loss: 0.7810 - val_mae: 0.1236 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 1.3669 - mae: 0.6336 - val_loss: 0.5866 - val_mae: 0.1033 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.9584 - mae: 0.4972 - val_loss: 0.4390 - val_mae: 0.1159 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.7095 - mae: 0.4251 - val_loss: 0.3177 - val_mae: 0.0684 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.5135 - mae: 0.3657 - val_loss: 0.2252 - val_mae: 0.0348 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.3737 - mae: 0.3188 - val_loss: 0.1606 - val_mae: 0.0369 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.2691 - mae: 0.2753 - val_loss: 0.1130 - val_mae: 0.0486 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.1887 - mae: 0.2339 - val_loss: 0.0776 - val_mae: 0.0395 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.1301 - mae: 0.2000 - val_loss: 0.0530 - val_mae: 0.0281 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0925 - mae: 0.1712 - val_loss: 0.0365 - val_mae: 0.0416 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0625 - mae: 0.1425 - val_loss: 0.0250 - val_mae: 0.0418 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0402 - mae: 0.1086 - val_loss: 0.0169 - val_mae: 0.0289 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0264 - mae: 0.0822 - val_loss: 0.0134 - val_mae: 0.0490 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0170 - mae: 0.0638 - val_loss: 0.0089 - val_mae: 0.0330 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0111 - mae: 0.0511 - val_loss: 0.0062 - val_mae: 0.0270 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0081 - mae: 0.0484 - val_loss: 0.0045 - val_mae: 0.0212 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0059 - mae: 0.0440 - val_loss: 0.0034 - val_mae: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0042 - mae: 0.0374 - val_loss: 0.0027 - val_mae: 0.0176 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0040 - mae: 0.0392 - val_loss: 0.0024 - val_mae: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0034 - mae: 0.0380 - val_loss: 0.0022 - val_mae: 0.0165 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0025 - mae: 0.0316 - val_loss: 0.0021 - val_mae: 0.0168 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0022 - mae: 0.0299 - val_loss: 0.0018 - val_mae: 0.0196 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0019 - mae: 0.0255 - val_loss: 0.0018 - val_mae: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0019 - mae: 0.0262 - val_loss: 0.0018 - val_mae: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0016 - mae: 0.0227 - val_loss: 0.0017 - val_mae: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0015 - mae: 0.0213 - val_loss: 0.0018 - val_mae: 0.0250 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0015 - mae: 0.0205 - val_loss: 0.0017 - val_mae: 0.0199 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0014 - mae: 0.0194 - val_loss: 0.0017 - val_mae: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0014 - mae: 0.0188 - val_loss: 0.0017 - val_mae: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:28:14.017717: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-14 04:28:14.109788: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - ETA: 0s - loss: 3.3162 - mae: 1.1052"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:28:23.199004: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 11s 20ms/step - loss: 3.3162 - mae: 1.1052 - val_loss: 1.1561 - val_mae: 0.1430 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 2.3188 - mae: 0.9131 - val_loss: 0.8503 - val_mae: 0.0426 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 1.5928 - mae: 0.7386 - val_loss: 0.6502 - val_mae: 0.0285 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 1.1326 - mae: 0.6153 - val_loss: 0.4942 - val_mae: 0.0270 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.8071 - mae: 0.5017 - val_loss: 0.3723 - val_mae: 0.0311 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.5840 - mae: 0.4139 - val_loss: 0.2759 - val_mae: 0.0365 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.4295 - mae: 0.3548 - val_loss: 0.2035 - val_mae: 0.0413 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.3135 - mae: 0.3018 - val_loss: 0.1497 - val_mae: 0.0427 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.2290 - mae: 0.2536 - val_loss: 0.1092 - val_mae: 0.0326 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.1560 - mae: 0.1957 - val_loss: 0.0806 - val_mae: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0991 - mae: 0.1304 - val_loss: 0.0616 - val_mae: 0.0153 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0710 - mae: 0.1003 - val_loss: 0.0503 - val_mae: 0.0586 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.0533 - mae: 0.0818 - val_loss: 0.0379 - val_mae: 0.0350 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0410 - mae: 0.0685 - val_loss: 0.0294 - val_mae: 0.0241 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0323 - mae: 0.0598 - val_loss: 0.0236 - val_mae: 0.0218 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0266 - mae: 0.0584 - val_loss: 0.0187 - val_mae: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0226 - mae: 0.0624 - val_loss: 0.0152 - val_mae: 0.0344 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0183 - mae: 0.0609 - val_loss: 0.0109 - val_mae: 0.0169 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.0149 - mae: 0.0590 - val_loss: 0.0083 - val_mae: 0.0231 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0108 - mae: 0.0483 - val_loss: 0.0062 - val_mae: 0.0215 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0077 - mae: 0.0389 - val_loss: 0.0047 - val_mae: 0.0239 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.0057 - mae: 0.0342 - val_loss: 0.0033 - val_mae: 0.0180 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.0044 - mae: 0.0317 - val_loss: 0.0024 - val_mae: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.0034 - mae: 0.0297 - val_loss: 0.0018 - val_mae: 0.0193 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.0027 - mae: 0.0271 - val_loss: 0.0014 - val_mae: 0.0206 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0023 - mae: 0.0246 - val_loss: 0.0011 - val_mae: 0.0183 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.0020 - mae: 0.0226 - val_loss: 0.0010 - val_mae: 0.0186 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0018 - mae: 0.0213 - val_loss: 9.6608e-04 - val_mae: 0.0191 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0017 - mae: 0.0205 - val_loss: 9.4665e-04 - val_mae: 0.0199 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0017 - mae: 0.0200 - val_loss: 8.8955e-04 - val_mae: 0.0182 - lr: 1.0000e-04\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:33:11.355401: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-14 04:33:11.447776: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - ETA: 0s - loss: 2.4636 - mae: 0.8677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:33:21.849649: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 12s 22ms/step - loss: 2.4636 - mae: 0.8677 - val_loss: 1.3158 - val_mae: 0.3876 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "504/504 [==============================] - 11s 21ms/step - loss: 1.6978 - mae: 0.7001 - val_loss: 0.9663 - val_mae: 0.3957 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 1.1841 - mae: 0.5760 - val_loss: 0.6898 - val_mae: 0.3574 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "504/504 [==============================] - 10s 21ms/step - loss: 0.8193 - mae: 0.4758 - val_loss: 0.4743 - val_mae: 0.2984 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.5564 - mae: 0.3913 - val_loss: 0.3234 - val_mae: 0.2570 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "504/504 [==============================] - 10s 21ms/step - loss: 0.3668 - mae: 0.3184 - val_loss: 0.2193 - val_mae: 0.2303 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "504/504 [==============================] - 10s 21ms/step - loss: 0.2356 - mae: 0.2585 - val_loss: 0.1486 - val_mae: 0.2138 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "504/504 [==============================] - 12s 23ms/step - loss: 0.1413 - mae: 0.1997 - val_loss: 0.0820 - val_mae: 0.1495 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "504/504 [==============================] - 11s 21ms/step - loss: 0.0786 - mae: 0.1443 - val_loss: 0.0478 - val_mae: 0.1236 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "504/504 [==============================] - 11s 21ms/step - loss: 0.0431 - mae: 0.1043 - val_loss: 0.0177 - val_mae: 0.0381 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0246 - mae: 0.0827 - val_loss: 0.0086 - val_mae: 0.0278 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0140 - mae: 0.0666 - val_loss: 0.0043 - val_mae: 0.0213 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0084 - mae: 0.0536 - val_loss: 0.0026 - val_mae: 0.0202 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "504/504 [==============================] - 10s 21ms/step - loss: 0.0053 - mae: 0.0446 - val_loss: 0.0019 - val_mae: 0.0170 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "504/504 [==============================] - 11s 21ms/step - loss: 0.0038 - mae: 0.0382 - val_loss: 0.0016 - val_mae: 0.0221 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0035 - mae: 0.0396 - val_loss: 0.0014 - val_mae: 0.0194 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "504/504 [==============================] - 10s 21ms/step - loss: 0.0032 - mae: 0.0390 - val_loss: 0.0013 - val_mae: 0.0193 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0025 - mae: 0.0319 - val_loss: 0.0013 - val_mae: 0.0194 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0019 - mae: 0.0251 - val_loss: 0.0013 - val_mae: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0017 - mae: 0.0217 - val_loss: 0.0012 - val_mae: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0016 - mae: 0.0201 - val_loss: 0.0012 - val_mae: 0.0201 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0015 - mae: 0.0193 - val_loss: 0.0012 - val_mae: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "502/504 [============================>.] - ETA: 0s - loss: 0.0016 - mae: 0.0195\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "504/504 [==============================] - 10s 21ms/step - loss: 0.0016 - mae: 0.0195 - val_loss: 0.0012 - val_mae: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "504/504 [==============================] - 11s 21ms/step - loss: 0.0016 - mae: 0.0198 - val_loss: 0.0012 - val_mae: 0.0168 - lr: 5.0000e-05\n",
      "Epoch 25/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0016 - mae: 0.0194 - val_loss: 0.0012 - val_mae: 0.0175 - lr: 5.0000e-05\n",
      "Epoch 26/30\n",
      "504/504 [==============================] - 11s 21ms/step - loss: 0.0015 - mae: 0.0189 - val_loss: 0.0012 - val_mae: 0.0174 - lr: 5.0000e-05\n",
      "Epoch 27/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0015 - mae: 0.0187 - val_loss: 0.0012 - val_mae: 0.0170 - lr: 5.0000e-05\n",
      "Epoch 28/30\n",
      "502/504 [============================>.] - ETA: 0s - loss: 0.0015 - mae: 0.0186\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0015 - mae: 0.0186 - val_loss: 0.0012 - val_mae: 0.0174 - lr: 5.0000e-05\n",
      "Epoch 29/30\n",
      "504/504 [==============================] - 10s 21ms/step - loss: 0.0015 - mae: 0.0186 - val_loss: 0.0012 - val_mae: 0.0177 - lr: 2.5000e-05\n",
      "Epoch 30/30\n",
      "504/504 [==============================] - 11s 21ms/step - loss: 0.0015 - mae: 0.0186 - val_loss: 0.0012 - val_mae: 0.0175 - lr: 2.5000e-05\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:38:26.129783: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-14 04:38:26.221533: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - ETA: 0s - loss: 2.7216 - mae: 0.9793"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 04:38:35.599257: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 11s 20ms/step - loss: 2.7216 - mae: 0.9793 - val_loss: 1.2119 - val_mae: 0.3155 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 1.6522 - mae: 0.6900 - val_loss: 0.8343 - val_mae: 0.0450 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 1.2001 - mae: 0.5772 - val_loss: 0.6162 - val_mae: 0.0370 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.8079 - mae: 0.4207 - val_loss: 0.4644 - val_mae: 0.1257 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.5555 - mae: 0.3189 - val_loss: 0.3367 - val_mae: 0.1206 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.4131 - mae: 0.2847 - val_loss: 0.2325 - val_mae: 0.0713 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.3002 - mae: 0.2514 - val_loss: 0.1729 - val_mae: 0.1232 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.2152 - mae: 0.2181 - val_loss: 0.1129 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.1559 - mae: 0.1940 - val_loss: 0.0775 - val_mae: 0.0684 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "504/504 [==============================] - 10s 19ms/step - loss: 0.1131 - mae: 0.1742 - val_loss: 0.0626 - val_mae: 0.1153 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0794 - mae: 0.1553 - val_loss: 0.0355 - val_mae: 0.0425 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0575 - mae: 0.1376 - val_loss: 0.0251 - val_mae: 0.0380 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0410 - mae: 0.1158 - val_loss: 0.0221 - val_mae: 0.0739 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 0.0275 - mae: 0.0920 - val_loss: 0.0145 - val_mae: 0.0493 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "504/504 [==============================] - 11s 21ms/step - loss: 0.0195 - mae: 0.0779 - val_loss: 0.0105 - val_mae: 0.0423 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0125 - mae: 0.0599 - val_loss: 0.0070 - val_mae: 0.0257 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0088 - mae: 0.0520 - val_loss: 0.0048 - val_mae: 0.0166 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0064 - mae: 0.0469 - val_loss: 0.0040 - val_mae: 0.0234 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0049 - mae: 0.0418 - val_loss: 0.0026 - val_mae: 0.0175 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0037 - mae: 0.0378 - val_loss: 0.0023 - val_mae: 0.0164 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0027 - mae: 0.0320 - val_loss: 0.0020 - val_mae: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0022 - mae: 0.0278 - val_loss: 0.0016 - val_mae: 0.0211 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0019 - mae: 0.0241 - val_loss: 0.0015 - val_mae: 0.0162 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0017 - mae: 0.0218 - val_loss: 0.0018 - val_mae: 0.0187 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0017 - mae: 0.0215 - val_loss: 0.0015 - val_mae: 0.0160 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0016 - mae: 0.0207 - val_loss: 0.0014 - val_mae: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0015 - mae: 0.0200 - val_loss: 0.0014 - val_mae: 0.0173 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0015 - mae: 0.0198 - val_loss: 0.0015 - val_mae: 0.0225 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0016 - mae: 0.0208 - val_loss: 0.0014 - val_mae: 0.0179 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "502/504 [============================>.] - ETA: 0s - loss: 0.0015 - mae: 0.0190\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "504/504 [==============================] - 10s 20ms/step - loss: 0.0015 - mae: 0.0191 - val_loss: 0.0014 - val_mae: 0.0178 - lr: 1.0000e-04\n",
      "Average Training Loss (MSE) across 5 folds: 0.001487854216247797\n",
      "Average Validation Loss (MSE) across 5 folds: 0.0014190083718858659\n",
      "Average Validation MAE across 5 folds: 0.018118299171328546\n"
     ]
    }
   ],
   "source": [
    "# Define the Transformer model\n",
    "def create_transformer_model(input_shape, num_heads, ff_dim, num_classes):\n",
    "    inputs = layers.Input(shape=(input_shape[0], input_shape[1]))  # (sequence_length, num_features)\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[1])(inputs, inputs)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual connection\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(x)\n",
    "    x = layers.Conv1D(filters=input_shape[1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)  # Residual connection\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Add a Dense layer with increased L2 regularization and Dropout\n",
    "    units = 16  # Reduced number of units\n",
    "    x = layers.Dense(units, activation='relu', kernel_regularizer=l2(1e-1))(x)  # Increased L2 regularization\n",
    "    x = layers.Dropout(0.4)(x)  # Added Dropout layer\n",
    "    x = layers.BatchNormalization()(x)  # Added Batch Normalization\n",
    "\n",
    "        \n",
    "    # Output layer - no activation for regression\n",
    "    outputs = layers.Dense(num_classes)(x)  # Predicting total sales as a single value\n",
    "    \n",
    "    TR_model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return TR_model\n",
    "    \n",
    "# Parameters\n",
    "input_shape = (TR_X_train_scaled.shape[1], TR_X_train_scaled.shape[2])  # (sequence_length, num_features)\n",
    "num_heads = 2\n",
    "ff_dim = 16\n",
    "num_classes = 1  # Predicting total sales as a single value\n",
    "\n",
    "\n",
    "# Define a custom learning rate\n",
    "learning_rate = 0.0001  # Consider using a higher value\n",
    "\n",
    "# Create KFold object\n",
    "k = 5  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "fold_losses = []\n",
    "fold_maes = []\n",
    "\n",
    "# Initialize lists to store loss values for each fold\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "for train_index, val_index in kf.split(TR_X_train_scaled):\n",
    "    X_train_fold, X_val_fold = TR_X_train[train_index], TR_X_train[val_index]\n",
    "    y_train_fold, y_val_fold = TR_y_train[train_index], TR_y_train[val_index]\n",
    "\n",
    "    # Create the Transformer model\n",
    "    TR_model = create_transformer_model(input_shape, num_heads, ff_dim, num_classes)\n",
    "\n",
    "    # Use the Adam optimizer with a custom learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)  \n",
    "\n",
    "    # Compile the model with the custom optimizer\n",
    "    TR_model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    # Define the learning rate scheduler\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the Transformer model with early stopping\n",
    "    history = TR_model.fit(X_train_fold, y_train_fold, epochs=30, batch_size=32, \n",
    "                        validation_data=(X_val_fold, y_val_fold), \n",
    "                        callbacks=[early_stopping, lr_scheduler], verbose=1) \n",
    "    \n",
    "    # Append the training and validation loss for this fold\n",
    "    all_train_losses.append(history.history['loss'])\n",
    "    all_val_losses.append(history.history['val_loss'])\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_mae = TR_model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    fold_losses.append(val_loss)\n",
    "    fold_maes.append(val_mae)\n",
    "\n",
    "\n",
    "# Average the losses across all folds\n",
    "avg_train_loss = np.mean(all_train_losses, axis=0)\n",
    "avg_val_loss = np.mean(all_val_losses, axis=0)\n",
    "\n",
    "# Print average results across all folds\n",
    "print(f'Average Training Loss (MSE) across {k} folds: {avg_train_loss[-1]}')  # Last epoch's average training loss\n",
    "print(f'Average Validation Loss (MSE) across {k} folds: {avg_val_loss[-1]}')  # Last epoch's average validation loss\n",
    "print(f'Average Validation MAE across {k} folds: {np.mean(fold_maes)}')  # Overall average MAE across folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfd00d90-64a4-440a-a36d-107fb1fa6771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "630/630 [==============================] - ETA: 0s - loss: 0.0014 - mae: 0.0184WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 12s 18ms/step - loss: 0.0014 - mae: 0.0184\n",
      "Epoch 2/30\n",
      "629/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0181WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0181\n",
      "Epoch 3/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0179WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0179\n",
      "Epoch 4/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0180WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0180\n",
      "Epoch 5/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0180WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0180\n",
      "Epoch 6/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0179WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0178\n",
      "Epoch 7/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0177WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0177\n",
      "Epoch 8/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0177WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0177\n",
      "Epoch 9/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0176WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0176\n",
      "Epoch 10/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0177WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0177\n",
      "Epoch 11/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0177WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0177\n",
      "Epoch 12/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0176WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0176\n",
      "Epoch 13/30\n",
      "630/630 [==============================] - ETA: 0s - loss: 0.0014 - mae: 0.0176WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0176\n",
      "Epoch 14/30\n",
      "630/630 [==============================] - ETA: 0s - loss: 0.0014 - mae: 0.0176WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0176\n",
      "Epoch 15/30\n",
      "630/630 [==============================] - ETA: 0s - loss: 0.0014 - mae: 0.0176WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0176\n",
      "Epoch 16/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0174WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0174\n",
      "Epoch 17/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0176WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0176\n",
      "Epoch 18/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0175\n",
      "Epoch 19/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0176WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0176\n",
      "Epoch 20/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0174WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0174\n",
      "Epoch 21/30\n",
      "629/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 12s 19ms/step - loss: 0.0014 - mae: 0.0175\n",
      "Epoch 22/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0175\n",
      "Epoch 23/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0174WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 18ms/step - loss: 0.0014 - mae: 0.0174\n",
      "Epoch 24/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0176WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 17ms/step - loss: 0.0014 - mae: 0.0176\n",
      "Epoch 25/30\n",
      "629/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0174WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 17ms/step - loss: 0.0014 - mae: 0.0174\n",
      "Epoch 26/30\n",
      "628/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0174WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 17ms/step - loss: 0.0014 - mae: 0.0174\n",
      "Epoch 27/30\n",
      "629/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0174WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 17ms/step - loss: 0.0014 - mae: 0.0174\n",
      "Epoch 28/30\n",
      "629/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 17ms/step - loss: 0.0014 - mae: 0.0175\n",
      "Epoch 29/30\n",
      "629/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 11s 17ms/step - loss: 0.0014 - mae: 0.0175\n",
      "Epoch 30/30\n",
      "629/630 [============================>.] - ETA: 0s - loss: 0.0014 - mae: 0.0175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "630/630 [==============================] - 12s 19ms/step - loss: 0.0014 - mae: 0.0175\n",
      "Training MSE (last epoch): 0.001487854216247797\n",
      "Validation MSE: 0.01175004243850708\n",
      "Test Loss MSE: 0.0034693244379013777\n",
      "Test MAE: 0.02673877775669098\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the entire training set after K-Fold Cross Validation\n",
    "TR_model.fit(TR_X_train_scaled, TR_y_train, epochs=30, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_mae = TR_model.evaluate(TR_X_val_scaled, TR_y_val, verbose=0)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = TR_model.evaluate(TR_X_test_scaled, TR_y_test, verbose=0)\n",
    "\n",
    "# Print the outcomes\n",
    "print(f'Training MSE (last epoch): {avg_train_loss[-1]}')  # Assuming avg_train_loss is calculated during K-Fold\n",
    "print(f'Validation MSE: {val_loss}')  # Validation loss (MSE)\n",
    "print(f'Test Loss MSE: {test_loss}')  # Test loss (MSE)\n",
    "print(f'Test MAE: {test_mae}')  # Test MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bc6db09-dd2f-4f2a-9cc6-db1e776d0cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Transformer Average Training Loss: 0.001487854216247797\n",
      "Final Transformer Average Validation Loss: 0.0014190083718858659\n",
      "Final Transformer Average Validation Loss Across Folds: 0.0014190083718858659\n"
     ]
    }
   ],
   "source": [
    "# Plot training and validation loss values for the Transformer model\n",
    "plt.plot(avg_train_loss, label='Average Training Loss')\n",
    "plt.plot(avg_val_loss, label='Average Validation Loss')\n",
    "plt.axhline(y=np.mean(fold_losses), color='r', linestyle='--', label='Average Validation Loss Across Folds')  # Add average validation loss as a horizontal line\n",
    "plt.title('Transformer Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print final loss values for comparison\n",
    "print(f'Final Transformer Average Training Loss: {avg_train_loss[-1]}')\n",
    "print(f'Final Transformer Average Validation Loss: {avg_val_loss[-1]}')\n",
    "print(f'Final Transformer Average Validation Loss Across Folds: {np.mean(fold_losses)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f418703-9686-415c-890c-cbdbd0d775ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (33558, 72)\n",
      "Target shape: (33558,)\n"
     ]
    }
   ],
   "source": [
    "def create_selected_lagged_features(X, y, lags):\n",
    "  \n",
    "    \n",
    "    # Ensure X is a DataFrame\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    # Ensure y is a Series\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = pd.Series(y)\n",
    "    \n",
    "    # Initialize list with the original features\n",
    "    lagged_dfs = [X]\n",
    "    \n",
    "    # Create and append the lagged features for each specified lag\n",
    "    for lag in lags:\n",
    "        lagged_df = X.shift(lag).add_suffix(f'_lag{lag}')\n",
    "        lagged_dfs.append(lagged_df)\n",
    "    \n",
    "    # Concatenate all features along columns and drop rows with missing values\n",
    "    X_lagged = pd.concat(lagged_dfs, axis=1).dropna()\n",
    "    \n",
    "    # Align the target variable y with the lagged features by selecting the matching indices\n",
    "    y_lagged = y.loc[X_lagged.index]\n",
    "    \n",
    "    return X_lagged, y_lagged\n",
    "\n",
    "\n",
    "# Compute Partial Autocorrelation for first 40 lags\n",
    "pacf_values = pacf(y, nlags=40)  \n",
    "\n",
    "# Select lags where PACF is significant (threshold: 0.1)\n",
    "selected_lags = [i for i, val in enumerate(pacf_values) if abs(val) > 0.1]\n",
    "\n",
    "# Use these selected lags in the function\n",
    "X_lagged, y_lagged = create_selected_lagged_features(X, y, lags=selected_lags)\n",
    "\n",
    "\n",
    "print(f'Features shape: {X_lagged.shape}')\n",
    "print(f'Target shape: {y_lagged.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e9a8ebe-7ff9-4cee-a11e-894804c59b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 20134\n",
      "Validation set size: 6711\n",
      "Test set size: 6713\n"
     ]
    }
   ],
   "source": [
    "# Follow chronological splitting as Transformer model\n",
    "\n",
    "train_size = int(len(X_lagged) * 0.6)\n",
    "val_size = int(len(X_lagged) * 0.2)\n",
    "test_size = len(X_lagged) - train_size - val_size\n",
    "\n",
    "X_train, y_train = X_lagged[:train_size], y_lagged[:train_size]\n",
    "X_val, y_val = X_lagged[train_size:train_size + val_size], y_lagged[train_size:train_size + val_size]\n",
    "X_test, y_test = X_lagged[train_size + val_size:], y_lagged[train_size + val_size:]\n",
    "\n",
    "\n",
    "# Check the sizes of the splits\n",
    "print(f'Training set size: {X_train.shape[0]}')\n",
    "print(f'Validation set size: {X_val.shape[0]}')\n",
    "print(f'Test set size: {X_test.shape[0]}')\n",
    "\n",
    "# Flatten the input sequences for Random Forest and linear regression\n",
    "X_train_flat = X_train.values.reshape(X_train.shape[0], -1)  # Convert to NumPy array and reshape\n",
    "X_val_flat = X_val.values.reshape(X_val.shape[0], -1)        # Convert to NumPy array and reshape\n",
    "X_test_flat = X_test.values.reshape(X_test.shape[0], -1)     # Convert to NumPy array and reshape\n",
    "\n",
    "# Initialize the scaler for the target variable\n",
    "scaler_y = MinMaxScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()  # Convert Series to NumPy array\n",
    "y_val = scaler_y.transform(y_val.values.reshape(-1, 1)).flatten()          # Convert Series to NumPy array\n",
    "y_test = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()        # Convert Series to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a572d368-de5e-44e7-a0c8-d58df11af6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Parameters (Random Search): {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 7, 'n_estimators': 200}\n",
      "Random Forest Training MSE: 0.0012212053532497465\n",
      "Random Forest Validation MSE: 0.011791053701086952\n",
      "Random Forest Test Loss MSE: 0.003493794373360051, Test MAE: 0.025392824542438006\n"
     ]
    }
   ],
   "source": [
    "#Create and train the Random Forest model\n",
    "\n",
    "# Define the model\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Define the hyperparameters and their distributions to sample from\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [None, 5, 10, 15],  # Maximum depth of the trees\n",
    "    'min_samples_split': randint(2, 11),  # Random integer between 2 and 10\n",
    "    'min_samples_leaf': randint(1, 5)      # Random integer between 1 and 4\n",
    "}\n",
    "\n",
    "# Set up the Random Search\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, \n",
    "                                   n_iter=100, scoring='neg_mean_squared_error', \n",
    "                                   cv=5, verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit the Random Search to the data\n",
    "random_search.fit(X_train_flat, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_random = random_search.best_params_\n",
    "\n",
    "print(f'Best Parameters (Random Search): {best_params_random}')\n",
    "\n",
    "\n",
    "# Retrain the model with the best parameters\n",
    "final_rf_model = RandomForestRegressor(**best_params_random)\n",
    "final_rf_model.fit(X_train_flat, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on training, validation, and test sets\n",
    "train_predictions_rf = final_rf_model.predict(X_train_flat)\n",
    "val_predictions_rf = final_rf_model.predict(X_val_flat)\n",
    "rf_predictions = final_rf_model.predict(X_test_flat)\n",
    "\n",
    "\n",
    "rf_train_mse = mean_squared_error(y_train, train_predictions_rf)\n",
    "rf_val_mse = mean_squared_error(y_val, val_predictions_rf)\n",
    "rf_test_mse = mean_squared_error(y_test, rf_predictions)\n",
    "\n",
    "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
    "\n",
    "# Print results\n",
    "print(f'Random Forest Training MSE: {rf_train_mse}')\n",
    "print(f'Random Forest Validation MSE: {rf_val_mse}')\n",
    "print(f'Random Forest Test Loss MSE: {rf_test_mse}, Test MAE: {rf_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8afd091-f672-4c72-9d7f-d5bc8ff3f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Training MSE: 0.0013429241041490528\n",
      "Linear Regression Validation MSE: 0.0117742569929738\n",
      "Linear Regression Test MSE: 0.021792789120586357\n",
      "Linear Regression Test MAE: 0.029795746426048656\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler_X_lr = MinMaxScaler()\n",
    "\n",
    "# Convert DataFrames to NumPy arrays before reshaping\n",
    "X_train_array = X_train.to_numpy()  # Convert to NumPy array\n",
    "X_val_array = X_val.to_numpy()      # Convert to NumPy array\n",
    "X_test_array = X_test.to_numpy()    # Convert to NumPy array\n",
    "\n",
    "# Fit on training set only\n",
    "scaler_X_lr.fit(X_train_array.reshape(-1, X_train_array.shape[-1]))\n",
    "\n",
    "# Transform and reshape the data\n",
    "X_train_lr_scaled = scaler_X_lr.transform(X_train_array.reshape(-1, X_train_array.shape[-1])).reshape(X_train_array.shape)\n",
    "X_val_lr_scaled = scaler_X_lr.transform(X_val_array.reshape(-1, X_val_array.shape[-1])).reshape(X_val_array.shape)\n",
    "X_test_lr_scaled = scaler_X_lr.transform(X_test_array.reshape(-1, X_test_array.shape[-1])).reshape(X_test_array.shape)\n",
    "\n",
    "# Create and train the Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_lr_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = linear_model.predict(X_train_lr_scaled)\n",
    "val_predictions = linear_model.predict(X_val_lr_scaled)\n",
    "test_predictions = linear_model.predict(X_test_lr_scaled)\n",
    "\n",
    "# Calculate MAE and MSE for Linear Regression\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "# Print results\n",
    "print(f'Linear Regression Training MSE: {train_mse}')\n",
    "print(f'Linear Regression Validation MSE: {val_mse}')\n",
    "print(f'Linear Regression Test MSE: {test_mse}')\n",
    "print(f'Linear Regression Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9909ffe6-7715-4d47-9510-75278bbed341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained Transformer model\n",
    "\n",
    "TR_model.save('transformer_sales_model.h5')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cfe1789a-52d1-4910-bb5c-637c6951021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler refitted and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "# Assuming `monthly_sales` is your training dataset\n",
    "X_train = monthly_sales[['stock_code', 'CPI', 'GDP', 'VALUE_OF_RETAIL_SALES', 'VALUE_OF_Internet_Sales'] + list(holiday_df.columns)].values\n",
    "\n",
    "# Fit the scaler on 13 features\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Save the scaler again\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Scaler refitted and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cffecb5-200b-40d3-a4ba-b066d403fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "Running on public URL: https://d81d9a6fa01240421d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d81d9a6fa01240421d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gradio as gr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1: Load the trained model and scaler\n",
    "# -----------------------------\n",
    "model = tf.keras.models.load_model(\"transformer_sales_model.h5\")\n",
    "\n",
    "scaler_path = \"scaler.pkl\"\n",
    "if not os.path.exists(scaler_path):\n",
    "    raise FileNotFoundError(f\"{scaler_path} does not exist. Please ensure it is in the correct location.\")\n",
    "if os.stat(scaler_path).st_size == 0:\n",
    "    raise ValueError(f\"{scaler_path} is empty. Please check that the scaler was saved correctly during training.\")\n",
    "\n",
    "# Load the scaler (fitted on 12 features: 4 numerical + 8 holiday features)\n",
    "with open(scaler_path, \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Define holiday options and stock codes\n",
    "# -----------------------------\n",
    "holiday_options = [\n",
    "    \"None\", \n",
    "    \"Back to School\", \n",
    "    \"Bank Holiday\", \n",
    "    \"Black Friday\", \n",
    "    \"Christmas\", \n",
    "    \"Easter\", \n",
    "    \"New Year's Day Holiday\", \n",
    "    \"Queen's Diamond Jubilee Holiday\"\n",
    "]\n",
    "\n",
    "# Assume `top_items` is defined elsewhere.\n",
    "# For demonstration, here is a dummy list:\n",
    "stock_codes = top_items['stock_code'].tolist()\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Helper functions\n",
    "# -----------------------------\n",
    "def convert_stock_code(stock_code):\n",
    "    \"\"\"Convert stock code to a numeric value (if needed elsewhere).\"\"\"\n",
    "    try:\n",
    "        return float(stock_code)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return float(int(stock_code, 36))\n",
    "        except Exception:\n",
    "            return float(sum(ord(c) for c in stock_code))\n",
    "\n",
    "def encode_holidays(holiday_column):\n",
    "    \"\"\"\n",
    "    One-hot encode each holiday.\n",
    "    Returns an array of shape (number_of_months, 8).\n",
    "    \"\"\"\n",
    "    return np.array([[1 if h == option else 0 for option in holiday_options] for h in holiday_column])\n",
    "\n",
    "def prepare_scaled_input(input_data, holidays):\n",
    "    \"\"\"\n",
    "    Prepare the input for inference.\n",
    "    - input_data: (10, 4) numerical features.\n",
    "    - holidays: list of 10 holiday selections.\n",
    "    Concatenates them into a (10, 12) array and applies scaling.\n",
    "    \"\"\"\n",
    "    input_array = np.array(input_data).astype(float)  # Expected shape: (10, 4)\n",
    "    holiday_encoded = encode_holidays(list(holidays))   # Expected shape: (10, 8)\n",
    "    full_input = np.hstack((input_array, holiday_encoded))  # Shape: (10, 12)\n",
    "    \n",
    "    # Debug: Verify dimensions\n",
    "    print(f\"prepare_scaled_input - Expected: (10, 12), Actual: {full_input.shape}\")\n",
    "    print(f\"Scaler expects {scaler.n_features_in_} features.\")  # Should be 12\n",
    "    \n",
    "    scaled_input = scaler.transform(full_input)  # Shape remains (10, 12)\n",
    "    return scaled_input\n",
    "\n",
    "def inverse_scale_output(scaled_prediction):\n",
    "    \"\"\"\n",
    "    Inverse-transform a single prediction back to original scale.\n",
    "    scaled_prediction: shape (1, 1)\n",
    "    \"\"\"\n",
    "    dummy = np.zeros((1, 12))\n",
    "    dummy[0, 0] = scaled_prediction.flatten()[0]\n",
    "    unscaled = scaler.inverse_transform(dummy)\n",
    "    return unscaled[0, 0]\n",
    "\n",
    "def inverse_scale_output_batch(scaled_predictions):\n",
    "    \"\"\"\n",
    "    Vectorized inverse scaling for a batch of predictions.\n",
    "    scaled_predictions: shape (N, 1)\n",
    "    Returns a 1D array of length N.\n",
    "    \"\"\"\n",
    "    N = scaled_predictions.shape[0]\n",
    "    dummy = np.zeros((N, 12))\n",
    "    dummy[:, 0] = scaled_predictions.flatten()\n",
    "    unscaled = scaler.inverse_transform(dummy)\n",
    "    return unscaled[:, 0]\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Prediction functions\n",
    "# -----------------------------\n",
    "def predict_per_product(input_data, stock_code, *holidays):\n",
    "    \"\"\"\n",
    "    Predict sales for a single product.\n",
    "    Note: The model input consists solely of 12 features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare scaled input (shape: (10, 12))\n",
    "        scaled_input = prepare_scaled_input(input_data, holidays)\n",
    "        final_input = scaled_input[np.newaxis, :, :]  # Reshape to (1, 10, 12)\n",
    "        scaled_prediction = model.predict(final_input)  # Expected shape: (1, 1)\n",
    "        prediction = inverse_scale_output(scaled_prediction)\n",
    "        return f\"Predicted Sales for Product {stock_code}: {prediction:.2f}\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def predict_total_sales(input_data, *holidays):\n",
    "    \"\"\"\n",
    "    Predict total sales by batching predictions for all products.\n",
    "    Instead of looping over each product, we replicate the input\n",
    "    for all products and predict in one go.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the base scaled input (shape: (10, 12))\n",
    "        scaled_input = prepare_scaled_input(input_data, holidays)\n",
    "        N = len(stock_codes)\n",
    "        # Replicate scaled_input for each product: (N, 10, 12)\n",
    "        batch_input = np.tile(scaled_input, (N, 1, 1))\n",
    "        scaled_predictions = model.predict(batch_input)  # Expected shape: (N, 1)\n",
    "        predictions = inverse_scale_output_batch(scaled_predictions)  # (N,)\n",
    "        total_sales = np.sum(predictions)\n",
    "        return f\"Total Predicted Sales: {total_sales:.2f}\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 5: Create the Gradio Interface\n",
    "# -----------------------------\n",
    "def gradio_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Sales Prediction Interface for Gift Haven\")\n",
    "        gr.Markdown(\"Select a prediction mode and provide input data to predict future sales.\")\n",
    "\n",
    "        # Mode Selection\n",
    "        prediction_mode = gr.Radio(\n",
    "            [\"Predict per product\", \"Predict total sales\"],\n",
    "            label=\"Select Prediction Mode\"\n",
    "        )\n",
    "\n",
    "        # Stock Code input (only used for per-product predictions)\n",
    "        stock_code = gr.Textbox(label=\"Stock Code (e.g., '20725' '22763')\", visible=False)\n",
    "\n",
    "        # Input Data for 10 months (4 numerical features)\n",
    "        gr.Markdown(\"### Input Data (Past 10 Months)\")\n",
    "        example_data = pd.DataFrame({\n",
    "            \"CPI Index (Base Year: 2015 = 100)\": [0] * 10,\n",
    "            \"GDP Index (Base Year: 2019 = 100)\": [0] * 10,\n",
    "            \"Retails Sales Index (Base Year: 2022=100)\": [0] * 10,\n",
    "            \"Internet Sales (£ Million)\": [0] * 10\n",
    "        }, index=[f\"Month {i+1}\" for i in range(10)])\n",
    "        input_df = gr.DataFrame(value=example_data, interactive=True, label=\"Enter past 10 months' data\")\n",
    "\n",
    "        # Holiday Selection for each month\n",
    "        gr.Markdown(\"### Select Holiday for Each Month\")\n",
    "        holiday_dropdowns = [\n",
    "            gr.Dropdown(holiday_options, label=f\"Month {i+1} - Holiday\", value=\"None\")\n",
    "            for i in range(10)\n",
    "        ]\n",
    "\n",
    "        # Prediction Button and Output\n",
    "        predict_button = gr.Button(\"Predict\")\n",
    "        output_text = gr.Textbox(label=\"Predicted Sales\")\n",
    "\n",
    "        # Show/hide stock code input based on mode selection\n",
    "        def update_ui(mode):\n",
    "            return gr.update(visible=(mode == \"Predict per product\"))\n",
    "        prediction_mode.change(update_ui, inputs=[prediction_mode], outputs=[stock_code])\n",
    "\n",
    "        # Prediction Handler\n",
    "        def predict_handler(mode, input_data, stock_code_value, *holidays):\n",
    "            if mode == \"Predict per product\":\n",
    "                return predict_per_product(input_data, stock_code_value, *holidays)\n",
    "            else:\n",
    "                return predict_total_sales(input_data, *holidays)\n",
    "\n",
    "        predict_button.click(\n",
    "            predict_handler,\n",
    "            inputs=[prediction_mode, input_df, stock_code] + holiday_dropdowns,\n",
    "            outputs=output_text\n",
    "        )\n",
    "    return demo\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 6: Launch the Gradio App\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    interface = gradio_interface()\n",
    "    interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e21be-74d4-4378-9fa1-01971a27ef80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env]",
   "language": "python",
   "name": "conda-env-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
